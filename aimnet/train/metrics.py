import torch
from torch import Tensor
import numpy as np
import ignite.distributed as idist
from ignite.metrics import Metric
from ignite.exceptions import NotComputableError
from ignite.metrics.metric import reinit__is_reduced
from typing import List, Dict
from collections import defaultdict
import logging


def regression_stats(pred, true):
    diff = true - pred
    diff2 = diff.pow(2)
    mae = diff.abs().mean(-1)
    rmse = diff2.mean(-1).sqrt()
    true_mean = true.mean()
    tot = (true - true_mean).pow(2).to(torch.double).sum()
    res = diff2.to(torch.double).sum(-1)
    r2 = 1 - (res / tot)
    return dict(mae=mae, rmse=rmse, r2=r2)


def cat_flatten(y_pred, y_true):
    if isinstance(y_true, (list, tuple)):
        y_true = torch.cat([x.view(-1) for x in y_true])
    if isinstance(y_pred, (list, tuple)):
        _n = sum(x.numel() for x in y_pred)
        assert not _n % y_true.numel()
        _npass = _n // y_true.numel()
        y_pred = torch.cat([x.view(_npass, -1) for x in y_pred], dim=1)
    y_true = y_true.view(-1)
    if y_pred.ndim > y_true.ndim:
        assert y_pred.ndim == y_true.ndim + 1
        if y_pred.shape[1] != y_true.shape[0]:
            y_pred = y_pred.view(-1, y_true.shape[0])
        _npass = y_pred.shape[0]
        y_pred = y_pred.view(_npass, -1)
    else:
        y_pred = y_pred.view(-1)
    return y_pred, y_true


def _iqr(a):
    a = a.view(-1)
    k1 = 1 + round(0.25 * (a.numel() - 1))
    k2 = 1 + round(0.75 * (a.numel() - 1))
    v1 = a.kthvalue(k1).values.item()
    v2 = a.kthvalue(k2).values.item()
    return v2 - v1


def _freedman_diaconis_bins(a, max_bins=50):
    """Calculate number of hist bins using Freedman-Diaconis rule."""
    # From https://stats.stackexchange.com/questions/798/
    if a.numel() < 2:
        return 1
    h = 2 * _iqr(a) / (a.numel() ** (1 / 3))
    # fall back to sqrt(a) bins if iqr is 0
    if h == 0:
        n_bins = int(np.sqrt(a.numel()))
    else:
        n_bins = int(np.ceil((a.max().item() - a.min().item()) / h))
    return min(n_bins, max_bins)


def calculate_metrics(result, histogram=False, corrplot=False):
    keys = [k[:-5] for k in result if k.endswith('_pred')]
    for k in keys:
        y_pred = result.pop(k + '_pred')
        y_true = result.pop(k + '_true')
        y_pred, y_true = cat_flatten(y_pred, y_true)
        stats = regression_stats(y_pred, y_true)
        npass = stats['mae'].numel()
        if k.split('.')[-1] in ('energy', 'forces'):
            f = 23.06  # eV to kcal/mol
        else:
            f = 1.0
        for i in range(npass):
            for m, v in stats.items():
                if m in ('mae', 'rmse'):
                    v[i] = v[i] * f
                result.log(f'{k}_{m}_{i}', v[i])
        if histogram:
            err = y_pred - y_true
            for i in range(npass):
                result[f'{k}_{i}_hist'] = torch.histc(err, bins=_freedman_diaconis_bins(err))
        #del result[k + '_pred']
        #del result[k + '_true']
    return result


class RegMultiMetric(Metric):
    def __init__(self, cfg : List[Dict], loss_fn=None):
        super().__init__()
        self.cfg = cfg
        self.loss_fn = loss_fn

    def attach_loss(self, loss_fn):
        self.loss_fn = loss_fn

    @reinit__is_reduced
    def reset(self):
        super().reset()
        self.data = defaultdict(lambda: defaultdict(float))
        self.atoms = 0.0
        self.samples = 0.0
        self.loss = 0.0

    def _update_one(self, key: str, pred: Tensor, true: Tensor) -> None:
        e = true - pred
        if pred.ndim > true.ndim:
            e = e.view(pred.shape[0], -1)
        else:
            e = e.view(-1)
        d = self.data[key]
        d['sum_abs_err'] += e.abs().sum(-1).to(dtype=torch.double, device='cpu')
        d['sum_sq_err'] += e.pow(2).sum(-1).to(dtype=torch.double, device='cpu')
        d['sum_true'] += true.sum().to(dtype=torch.double, device='cpu')
        d['sum_sq_true'] += true.pow(2).sum().to(dtype=torch.double, device='cpu')

    @reinit__is_reduced
    def update(self, output) -> None:
        y_pred, y_true = output
        if y_pred is None:
            return
        for k in y_pred:
            if k not in y_true:
                continue
            with torch.no_grad():
                self._update_one(k, y_pred[k].detach(), y_true[k].detach())
            b = y_true[k].shape[0]
        self.samples += b

        _n = y_pred['_natom']
        if _n.numel() > 1:
            self.atoms += _n.sum().item()
        else:
            self.atoms += y_pred['numbers'].shape[0] * y_pred['numbers'].shape[1]
        
#        if '_natom' in y_pred:
#            if y_pred['_natom'].numel() == 1 and y_pred['_natom'].item() != 0:
#                self.atoms += y_pred['_natom'].item()
#            else:
#                self.atoms += y_pred['_natom'].sum().item()
#        else:
#            self.atoms += y_pred['numbers'].shape[0] * y_pred['numbers'].shape[1]

        if self.loss_fn is not None:
            with torch.no_grad():
                loss = self.loss_fn(y_pred, y_true)
                if loss.numel() > 1:
                    loss = loss.mean()
                loss = loss.item()
                self.loss += loss * b

    def compute(self):
        if self.samples == 0:
            raise NotComputableError
        # Use custom reduction
        if idist.get_world_size() > 1:
            self.atoms = idist.all_reduce(self.atoms)
            self.samples = idist.all_reduce(self.samples)
            self.loss = idist.all_reduce(self.loss)
            for k1, v1 in self.data.items():
                for k2, v2 in v1.items():
                    self.data[k1][k2] = idist.all_reduce(v2)
        self._is_reduced = True

        # compute
        ret = dict()
        for k in self.data:
            if k not in self.cfg:
                continue
            cfg = self.cfg[k]
            _n = self.atoms if cfg.get('peratom', False) else self.samples
            _n *= cfg.get('mult', 1.0)
            name = k
            abbr = cfg['abbr']
            v = self.data[name]
            m = dict()
            m['mae'] = v['sum_abs_err'] / _n
            m['rmse'] = (v['sum_sq_err'] / _n).sqrt()
            m['r2'] = 1.0 - v['sum_sq_err'] / (v['sum_sq_true'] - (v['sum_true'].pow(2)) / _n)
            for k, v in m.items():
                if k in ('mae', 'rmse'):
                    v *= cfg.get('scale', 1.0)
                v = v.tolist()
                if isinstance(v, list):
                    for ii, vv in enumerate(v):
                        ret[f'{abbr}_{k}_{ii}'] = vv
                else:
                    ret[f'{abbr}_{k}'] = v
        if self.loss:
            ret['loss'] = self.loss / self.samples

        logging.info(str(ret))

        return ret

